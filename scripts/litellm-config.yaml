model_list:
  - model_name: anthropic-claude-4.5-sonnet
    litellm_params:
      model: openai/anthropic-claude-4.5-sonnet
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: anthropic-claude-opus-4.5
    litellm_params:
      model: openai/anthropic-claude-opus-4.5
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: anthropic-claude-opus-4.6
    litellm_params:
      model: openai/anthropic-claude-opus-4.6
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: anthropic-claude-sonnet-4
    litellm_params:
      model: openai/anthropic-claude-sonnet-4
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: openai-gpt-4.1
    litellm_params:
      model: openai/openai-gpt-4.1
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: openai-o3
    litellm_params:
      model: openai/openai-o3
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  - model_name: openai/anthropic-claude-opus-4.5
    litellm_params:
      model: openai/anthropic-claude-opus-4.5
      api_base: https://inference.do-ai.run/v1
      api_key: os.environ/DO_API_KEY
  # Disabled — context length exceeded (40960 max vs ~42744 with opencode system prompt)
  # - model_name: alibaba-qwen3-32b
  #   litellm_params:
  #     model: openai/alibaba-qwen3-32b
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — model not found on inference.do-ai.run
  # - model_name: anthropic-claude-3.7-sonnet
  #   litellm_params:
  #     model: openai/anthropic-claude-3.7-sonnet
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — vLLM backend lacks --enable-auto-tool-choice
  # - model_name: deepseek-r1-distill-llama-70b
  #   litellm_params:
  #     model: openai/deepseek-r1-distill-llama-70b
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — vLLM backend lacks --enable-auto-tool-choice
  # - model_name: llama3.3-70b-instruct
  #   litellm_params:
  #     model: openai/llama3.3-70b-instruct
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — 400 Bad Request from OpenAI API
  # - model_name: openai-gpt-5
  #   litellm_params:
  #     model: openai/openai-gpt-5
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — 404 Not Found from OpenAI API
  # - model_name: openai-gpt-5.1-codex-max
  #   litellm_params:
  #     model: openai/openai-gpt-5.1-codex-max
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY
  # Disabled — 400 Bad Request from OpenAI API
  # - model_name: openai-gpt-5.2
  #   litellm_params:
  #     model: openai/openai-gpt-5.2
  #     api_base: https://inference.do-ai.run/v1
  #     api_key: os.environ/DO_API_KEY

litellm_settings:
  drop_params: true
